# Description
This file describes the implementation, architectural details and tech stack. Much intended to be created and used by AI.


# Design goals
The TiddeliHome app allows the user to control a Home Assistant (HA) using bidirectional, real-time audio streaming to Gemini AI. The App connects directly to both Gemini AI and Home Assistant (no backend). The functionality in summary:  The user can press a microphone button and speak a command, for example "turn on all lights in the kitchen". The AI will send back a Home Assistant control structure at the same time it says, "turning on all lights in the kitchen". The App will execute the command directly on Home Assistant via its REST API.

Documentation for the Google techno: https://ai.google.dev/gemini-api/docs

HA websocket: https://developers.home-assistant.io/docs/api/websocket/

HA REST: https://developers.home-assistant.io/docs/api/rest/#post-apiservicesltdomainltservice

# Tech stack

## Frontend:
- **TypeScript** (no framework - vanilla TypeScript)
- HTML5
- CSS (with Tailwind CSS for styling)
- **Port**: 3000 (default, Vite dev server)

## Media & Assets:
- **Icons**: PWA icons in multiple sizes (192x192, 512x512)
- **Audio**: No pre-recorded audio assets (all audio is generated by Gemini AI)
- **Images**: Minimal UI graphics (if any, use SVG for scalability)

## PWA Features:
- **Basic PWA Implementation** (minimum for installability)
- **manifest.json**: App metadata, name, icons, theme colors, display mode
- **Service Worker**: Required for PWA installation (minimal implementation)
  - Basic service worker registration
  - No offline support needed
- **Install Prompt**: Browser install button/prompt
- **HTTPS Required**: All PWA features require secure context
- **Icons**: Multiple sizes for different devices (192x192, 512x512)

## Additional Web APIs:
- **Web Audio API** (for audio capture and playback)
- **ScriptProcessorNode** (for real-time audio chunk processing) - Note: deprecated, consider AudioWorklet for future
- **AudioContext** (separate contexts for input and output with different sample rates)
- **GainNode** (used to prevent audio feedback in input pipeline)
- **WebSocket API** (for Home Assistant configuration fetching)
- **Fetch API** (for direct communication with Home Assistant REST API for command execution)

## AI Integration:
- **Google Gemini Live API** (`gemini-2.5-flash-native-audio-preview-09-2025` model)
- **Function Calling** (for Home Assistant control structure generation)
- **Google Search Grounding** (enabled via `enableGrounding: true` - allows AI to answer general knowledge questions)
- **Input Audio Transcription** (enabled - displays user's spoken input in UI log)
- **Home Assistant Configuration Context**: Custom JSON structure with entities (entity_id, name, area, domain) sent to AI
- Audio format: PCM Int16, 16kHz input (browser-determined, ideally 16kHz), 24kHz output (fixed)

## Known Issues and Limitations

### Gemini Live API Preview Model Limitations

**Model:** `gemini-2.5-flash-native-audio-preview-09-2025`

The preview model has known limitations that affect function calling and audio responses:

#### 1. Missing Audio Responses After Function Calls

**Issue:** The model may not generate audio responses after receiving function call results, even though:
- The function response format is correct
- The WebSocket connection remains open
- The function results are successfully sent to Gemini
- The model accepts the function response without errors

**Symptoms:**
- User asks a question requiring a function call (e.g., "Is the lamp on?")
- AI correctly calls `get_home_assistant_state` function
- Function executes successfully and returns state data
- Function response is sent to Gemini successfully
- **No audio response is generated** - AI remains silent
- Connection stays open and AI continues to listen for new commands

**Technical Details:**
- Function response format: Uses `sendToolResponse({ functionResponses: [FunctionResponse] })` SDK method
- Function response structure: `{ name, id, response, thoughtSignature? }` (FunctionResponse object directly, not wrapped in clientContent)
- No thought signatures are present in function calls from this model (unlike other Gemini models)
- Message structure: Function calls come from `msg.toolCall.functionCalls[]` (top-level), not `msg.serverContent.modelTurn.toolCall`
- System instruction explicitly instructs AI to respond with audio after function calls, but model doesn't comply

**Workarounds Attempted:**
- Multiple function response formats (`toolResponse`, `functionResponse`, with/without `id` field)
- Explicit system instructions requiring audio responses after function calls
- Checking for thought signatures (none found in this model)
- Various response structures and formats

**Status:** This is a **known limitation of the preview model**, not an implementation issue. The function calling implementation is correct, but the model doesn't generate audio responses after receiving function results.

**References:**
- [Google AI Developers Forum - Inconsistent Response Behavior](https://discuss.ai.google.dev/t/inconsistent-response-behavior-in-gemini-2-5-flash-native-audio-preview-09-2025-voicebot/110825)
- [Google AI Developers Forum - Function Calling Issues](https://discuss.ai.google.dev/t/function-calling-is-not-working-for-gemini-2-5-flash-preview-native-audio-dialog/85317)
- [GitHub Issue - Function Calling Not Working](https://github.com/livekit/livekit/issues/3679)

#### 2. Inconsistent Function Calling Behavior

**Issue:** Function calling may not work consistently with this preview model.

**Symptoms:**
- Function calls may fail to execute
- Function calls may work intermittently
- Some function calls succeed while others fail

**Status:** Known limitation - monitor official channels for updates.

#### 3. Intermittent Audio Response Behavior

**Issue:** The model sometimes fails to respond to audio inputs, requiring multiple retries.

**Symptoms:**
- Model remains silent despite receiving valid audio input
- Multiple attempts may be needed before model responds
- Audio responses may stop mid-sentence

**Status:** Known limitation - consider implementing client-side Voice Activity Detection (VAD) as a workaround.

### Recommendations

1. **Monitor Official Channels:** Keep an eye on official Gemini API documentation and forums for updates regarding these issues.

2. **Consider Alternative Models:** If function calling and audio responses are critical, consider using other Gemini models that have demonstrated more consistent behavior (when available).

3. **Report Issues:** Report specific issues through official Google AI support channels to help prioritize fixes.

4. **Workarounds:** 
   - For missing audio after function calls: User can ask a follow-up question to trigger a response
   - For inconsistent behavior: Implement retry logic and error handling
   - For audio interruptions: Consider implementing client-side VAD

**Note:** These limitations are expected to be addressed in future model updates. The current implementation is correct and ready for when the model is updated.


# Architecture

## Overview

**Architecture Pattern: Frontend-Only Direct Connection**

The application uses a frontend-only architecture for simplicity and low latency:
- **Frontend connects directly to Gemini Live API** (low latency for audio streaming)
- **Frontend connects directly to Home Assistant REST API** (requires CORS to be enabled on HA)

```
Frontend (Browser)                    External Services
─────────────────                    ──────────────────
Audio Capture                                                            
    │                                                                    
    │ Direct Connection (Gemini Live API)                                
    ├───────────────────────────────────────────────────────────────────►
    │                                                                    │
    │                                                                    │ Gemini Live API
    │                                                                    │ (API key in frontend)
    │                                                                    │
    │                                                                    │ (function calls - HA control JSON)
    │                                                                    │
    │                                                                    │
    │ HA Config Request (WebSocket)                                      │
    ├───────────────────────────────────────────────────────────────────►
    │                                                                    │
    │                                                                    ▼
    │                                                          Home Assistant
    │                                                          (WebSocket API)
    │                                                                    │
    │                                                                    │ (config data)
    │                                                                    │
    │                                                                    │
    │ HA Config Response                                                │
    ◄───────────────────────────────────────────────────────────────────┘
    │                                                                    │
    │ Function Calls (HA control JSON)                                   │
    ├───────────────────────────────────────────────────────────────────►
    │                                                                    │
    │                                                                    ▼
    │                                                          Home Assistant
    │                                                          (REST API)
    │                                                                    │
    │                                                                    │ (command execution)
    │                                                                    │
    │                                                                    │
    │ Command Execution Results                                          │
    ◄───────────────────────────────────────────────────────────────────┘
Audio Playback (direct from Gemini)
```

**Key Points:**
- Frontend captures audio using Web Audio API (ScriptProcessorNode) and converts to PCM Int16 chunks
- **Audio Pipeline Details:**
  - Input AudioContext: Browser-determined sample rate (ideally 16kHz via getUserMedia constraints)
  - Output AudioContext: Fixed 24kHz sample rate
  - Input pipeline: sourceNode → processor → gainNode (volume 0) → inputAudioContext.destination
  - GainNode prevents audio feedback while maintaining proper audio graph connection
  - AudioContext resume logic handles suspended contexts on mobile devices
- Frontend connects directly to Gemini Live API using `@google/genai` package (low latency)
- **API Key Challenge:** Both Gemini API key and HA access token must be in frontend code (security trade-off for simplicity)
- **HTTPS Required:** Vite dev server configured with HTTPS (via `vite-plugin-mkcert`) for secure microphone access from network IPs
- **Home Assistant Configuration Flow:**
  1. User clicks button in UI to extract HA configuration
  2. Frontend connects to Home Assistant via WebSocket API
  3. Frontend fetches entity registry and area information via WebSocket
  4. Frontend converts/condenses HA data into custom JSON structure (entities with entity_id, name, area, domain)
  5. Custom JSON structure is stored in memory and displayed in debug panel
  6. When connecting to Gemini, frontend sends custom JSON structure as context
  7. Configuration is included in system instruction or as initial context
  8. AI has knowledge of all HA entities in simplified format and can make intelligent decisions
- **Manual Extraction:** Configuration extraction happens only when user clicks button (no automatic re-fetch)
- Gemini processes audio and returns:
  - Audio response (spoken confirmation) - played directly in frontend
  - Function calls (Home Assistant control structure as JSON) - format matches HA REST API service call structure
- Frontend executes function calls directly on Home Assistant REST API (no translation needed)
- Frontend receives execution results directly from HA REST API

**Technical Details:**
- Audio format: PCM Int16, 16kHz input (browser-determined, requested via getUserMedia constraints), 24kHz output (fixed)
- Audio resampling: Input audio is resampled to match browser's actual sample rate before creating PCM blob
- Frontend uses `@google/genai` package directly (same as prototype)
- Frontend handles all Home Assistant API communication directly
- **HA Config Communication**: Frontend → HA WebSocket API (for configuration extraction)
- **HA Command Communication**: Frontend → HA REST API (for command execution)
- **HA Config Flow**: 
  1. User clicks "Extract HA Config" button in UI
  2. Frontend connects to HA WebSocket API (`wss://{baseUrl}/api/websocket` - uses HTTPS/WSS)
  3. Frontend requests entity registry and area information via WebSocket
  4. Frontend converts raw HA data into custom JSON structure
  5. Custom JSON structure stored in memory and available for Gemini context
- **CORS Requirement**: Home Assistant must have CORS enabled in `configuration.yaml` to allow browser REST API requests. Must include HTTPS origins when accessing from network IPs:
- **WebSocket Requirement**: Home Assistant WebSocket API must be accessible from browser (typically no CORS needed for WebSocket)
- **HTTPS Configuration**: Vite dev server uses `vite-plugin-mkcert` for automatic HTTPS certificate generation, enabling secure access from network IPs

**Custom JSON Structure:**
- Frontend converts HA entity registry data into simplified JSON format:
  ```json
  {
    "entities": [
      {
        "entity_id": "light.kitchen_corner",
        "name": "Kitchen Corner Lamp",
        "area": "Kitchen",
        "domain": "light"
      },
      {
        "entity_id": "light.kitchen_ceiling",
        "name": "Kitchen Ceiling Light",
        "area": "Kitchen",
        "domain": "light"
      }
    ]
  }
  ```
- This structure is sent to AI as context (simplified, focused on essential entity information)

**Function Calling Schema:**
- AI receives custom JSON structure with entities (entity_id, name, area, domain) as context
- AI returns Home Assistant control structure as JSON matching HA REST API service call format:
  ```json
  {
    "domain": "light",
    "service": "turn_on",
    "target": {
      "entity_id": "light.kitchen_corner"
    }
  }
  ```
- AI determines appropriate service calls based on:
  - Available entities from custom JSON structure
  - Natural language command interpretation
  - Entity names, areas, and domains
- Frontend executes service calls directly on HA REST API (no translation layer needed)

**Home Assistant Configuration Extraction Strategy:**
- **Manual Extraction**: User clicks button in UI to extract HA configuration
- **WebSocket Connection**: Frontend connects to HA WebSocket API for configuration data
- **Data Conversion**: Raw HA entity registry and area data converted to custom JSON structure
- **Storage**: Custom JSON structure stored in browser memory (persists until page reload or new extraction)
- **No Automatic Re-fetch**: Configuration is only updated when user manually triggers extraction
- **Entity Filtering**: Configurable via `config.json` with label-based exceptions:
  - **Mandatory Domain List**: Domains must be specified in `config.json` (`functionCalling.domains`) - no wildcard option
  - **Label-Based Inclusion**: Entities are included if they have the `use-by-ai` label, even if their domain is not in the main list
  - **Label-Based Exclusion**: Any entity with the `no_use-by_ai` label is excluded regardless of domain
  - **Label Normalization**: Label comparison is normalized to handle both hyphens and underscores (e.g., "use-by-ai" matches "use_by_ai")
- **Benefits**: 
  - User controls when to refresh configuration
  - Simplified data structure for AI context
  - Reduced data transfer (only essential entity information)
  - Flexible entity selection via labels
  - No backend required
  - Depending on usage,  "switch" es can be configured to *not* to be generally included it generally but instead "use_by_ai" tag for switches that control lights

## Directory Structure

```
TiddeliHome/
├── frontend/                 # Frontend PWA application
│   ├── src/                  # TypeScript source files
│   │   ├── api/              # API clients for Home Assistant communication
│   │   │   ├── haRestApi.ts  # Home Assistant REST API client
│   │   │   └── haWebSocket.ts # Home Assistant WebSocket API client
│   │   ├── audio/            # Audio capture and playback utilities (if any)
│   │   ├── types/            # TypeScript type definitions
│   │   │   └── ha.ts         # Home Assistant type definitions
│   │   ├── utils/            # Utility functions (refactored modules)
│   │   │   ├── audioUtils.ts # Audio processing utilities
│   │   │   ├── configLoader.ts # Configuration loading and merging
│   │   │   ├── functionCallExtractor.ts # Extract function calls from Gemini messages
│   │   │   ├── functionResponseBuilder.ts # Build function response payloads
│   │   │   ├── geminiConfigBuilder.ts # Build Gemini API configuration
│   │   │   ├── haConfigManager.ts # Manage HA configuration extraction
│   │   │   ├── haUrlBuilder.ts # Build HA WebSocket URLs
│   │   │   ├── thoughtSignatureExtractor.ts # Extract thought signatures
│   │   │   ├── uiHelpers.ts # UI helper functions
│   │   │   └── uiLogger.ts # UI debug logging utilities
│   │   ├── index.html        # Main HTML file
│   │   └── main.ts           # Application entry point
│   ├── config/               # Application configuration files
│   │   ├── config.json       # Main configuration (audio, gemini, HA, UI settings)
│   │   └── system-instruction.txt # System instruction template for AI
│   ├── public/               # Static assets (icons, manifest, etc.)
│   │   ├── manifest.json     # PWA manifest
│   │   ├── service-worker.js # Service worker for PWA
│   │   └── icons/            # App icons
│   ├── .env                  # Environment variables (not committed)
│   ├── vite.config.ts        # Vite configuration (with HTTPS support)
│   ├── tsconfig.json         # TypeScript configuration
│   └── package.json          # Frontend dependencies
│
├── Prototype/                # Original PoC (for reference only)
├── design.md                 # This file
├── README.md                 # Project documentation
└── .gitignore                # Git ignore rules
```



# Version management and updates

## Versioning Strategy
- **Semantic Versioning**: Use semantic versioning (MAJOR.MINOR.PATCH)
  - Version in `frontend/package.json`
- **PWA Updates**: Service worker handles app updates and cache invalidation
- **Version Display**: Optional version display in UI (for debugging)

# Development tools

## Primary Tools
- **Vite** (frontend build tool and dev server)
  - Fast HMR (Hot Module Replacement)
  - TypeScript support
  - Environment variable injection at build time
  - Optimized production builds
  - **HTTPS Support**: Configured with `vite-plugin-mkcert` for automatic certificate generation
  - **Network Access**: Server configured with `host: '0.0.0.0'` to allow access from network IPs

## Build Process
- Frontend: Vite handles TypeScript compilation and bundling
- Environment variables from `.env` injected at build time via Vite
- Production build outputs to `dist/` directory

## Dependencies

### Frontend Dependencies
- `@google/genai` - Google Gemini Live API client
- `vite` - Build tool and dev server
- `typescript` - TypeScript compiler
- `tailwindcss` - CSS framework (optional)
- `autoprefixer` - CSS vendor prefixing (if using Tailwind)

## Testing Environment

## Port Configuration
- **Frontend**: Port 3000 (Vite dev server, default)
- Port can be configured via environment variables if needed

## Environment Variables

### Frontend (`.env` in `frontend/` directory)
```env
# Google Gemini API Key (required)
VITE_GEMINI_API_KEY=your_gemini_api_key_here

# Home Assistant Access Token (required)
VITE_HA_ACCESS_TOKEN=your_long_lived_access_token_here
```

**Note:** Other configuration settings (baseUrl, timeout, audio settings, etc.) are now in `frontend/config/config.json`.

**Notes:**
- All `.env` files are excluded from git (see `.gitignore`)
- Frontend env variables must be prefixed with `VITE_` to be accessible in the app
- Never commit actual API keys or tokens to version control
- Both Gemini API key and HA access token are stored in frontend (security trade-off for simplicity)

## Configuration Files

### Frontend Configuration

**`vite.config.ts`**
- TypeScript compilation
- Environment variable injection
- Build output configuration
- Dev server settings (port 3000, HMR, HTTPS enabled)
- HTTPS configuration via `vite-plugin-mkcert` (automatic certificate generation)
- Network access enabled (`host: '0.0.0.0'`) for access from other devices on network

**`tsconfig.json`**
- TypeScript compiler options
- Module resolution
- Target ES version
- Strict type checking enabled

**`tailwind.config.js`** 
- Content paths for purging unused CSS
- Theme customization
- Plugin configuration

**`public/manifest.json`** (PWA)
- App name, short name
- Icons (multiple sizes)
- Theme colors
- Display mode (standalone)
- Start URL
- Scope

**`public/service-worker.js`**
- Minimal service worker for PWA installation
- No caching or offline support required

**`config/config.json`** - Main Application Configuration
- **Audio Settings**: Input/output sample rates, buffer size, channels, format
- **Gemini Settings**: Model selection, API key, system instruction, grounding enablement
- **Home Assistant Settings**: Base URL, access token, timeouts (REST and WebSocket)
- **Function Calling Settings**: Allowed domains, service data properties
- **UI Settings**: Feedback duration, auto-disconnect delay, connection monitor restore delay

**`config/system-instruction.txt`** - System Instruction Template
- Base system instruction for the AI
- HA configuration is injected into this template when available
- Includes instructions for function calling, audio responses, and general behavior

**`src/utils/configLoader.ts`** - Configuration Loading
- Loads and merges configuration from multiple sources
- Priority: localStorage > environment variables > config.json > system-instruction.txt template
- Handles API key and access token from environment variables
- Merges HA configuration into system instruction template

**`src/utils/geminiConfigBuilder.ts`** - Gemini API Configuration Builder
- Builds system instruction from template and HA config
- Constructs function calling tool definitions
- Configures Gemini Live API connection parameters
- Enables input audio transcription and Google Search grounding

**`src/api/haWebSocket.ts`** - Home Assistant WebSocket Client
- WebSocket connection management
- Entity registry fetching (includes label information)
- Area information fetching
- Device registry fetching (for area mapping)
- Data conversion to custom JSON structure
- Entity filtering based on mandatory domain list and labels:
  - Uses domains from `config.json` (`functionCalling.domains`) - domains list is required
  - Includes entities with `use-by-ai` label even if their domain is not in the main list
  - Excludes entities with `no_use-by_ai` label (regardless of domain)
  - Label comparison handles both hyphen and underscore formats

**`src/api/haRestApi.ts`** - Home Assistant REST API Client
- Service call execution (`executeHAServiceCall`)
- Entity state querying (`getHAEntityState`)
- REST API URL normalization (handles trailing slashes)
- Error handling and logging

### Configuration Loading Strategy

**Frontend:**
- **Configuration Sources** (priority order):
  1. User settings in localStorage (highest priority)
  2. Environment variables (`.env` file, prefixed with `VITE_`)
  3. `config.json` file (default values)
  4. `system-instruction.txt` template (for system instruction)
- **Runtime Loading**: `configLoader.ts` handles merging from all sources
- **HA Configuration**: Fetched at runtime from HA WebSocket API when user clicks "Extract HA Config"
- **System Instruction**: Built dynamically by merging template with HA config (if available)
- **Configuration Caching**: HA config stored in browser memory until page reload or new extraction


## Optimization
- Tailwind CLI automatically purges unused CSS classes
- Generated CSS is minified for production
- Only classes actually used in HTML/JS are included in final CSS

# Error Handling and User Feedback

## Error Handling Strategy

**Approach: Simple Error Messages (MVP)**

- **User-Facing Errors:**
  - Toast/notification messages for user-visible errors
  - Simple, clear error messages (no technical jargon)
  - Visual indicators (e.g., red error banner, icon)
  
- **Error Categories:**
  - **Network Errors**: Connection failures, timeouts
  - **AI Errors**: Gemini API failures, audio processing issues
  - **Home Assistant Errors**: HA API failures, invalid commands
  - **Audio Errors**: Microphone permissions, audio device issues
  
- **Error Logging:**
  - Errors logged to browser console (development)
  
- **Retry Logic:**
  - Basic retry for network errors (1-2 retries)
  - User can manually retry failed operations
  - No automatic exponential backoff (keeps it simple)

- **User Feedback:**
  - Success indicators for completed commands
  - Error messages for failed operations
  - Loading states during processing
  - Connection status indicator (connected/disconnected)

# Testing

## Testing Strategy
- **Manual Testing**: Primary testing approach for MVP
- **Browser Testing**: Test on Chrome, Firefox, Safari (PWA compatibility)
- **Audio Testing**: Test microphone permissions and audio streaming
- **Integration Testing**: Test frontend-Gemini-HA communication flow
- **CORS Testing**: Verify Home Assistant CORS configuration works correctly

# Security and Privacy

## Security Considerations

### API Key Management

**Approach: Environment Variables at Build Time**

Frontend needs direct connection to Gemini (low latency), but API key cannot be exposed in source code.

**Implementation:**
- API key stored in `.env` file (not committed to git)
- `.gitignore` configured to exclude all `.env*` files
- Build process injects key into frontend bundle at build time via Vite
- Key is in compiled JavaScript, but not in source code

**Security Mitigations:**
- Restrict API key to specific domains/IPs in Google Cloud Console
- Set API key quotas/rate limits
- Rotate keys periodically
- For personal/home use, risk is acceptable
- `.env` files are explicitly excluded in `.gitignore` to prevent accidental commits

**Home Assistant Token:**
- Stored in frontend environment variable (security trade-off for simplicity)
- Token is in compiled JavaScript bundle (similar to Gemini API key)
- Frontend executes function calls directly on HA API
- **CORS Requirement**: Home Assistant must have CORS enabled in `configuration.yaml`. Must include HTTPS origins when accessing from network IPs:
  ```yaml
  http:
    cors_allowed_origins:
      - https://localhost:3000
      - https://127.0.0.1:3000
      - https://192.168.1.204:3000  # Example: your dev machine IP
      # Add production URL when deployed
  ```

### Communication Security
- All WebSocket connections should use WSS (secure WebSocket) in production
- HTTPS required for PWA installation and service worker
- CORS must be configured on Home Assistant to allow browser requests

### Data Privacy
- Audio streams are processed in real-time, not stored
- Command logs may be stored for debugging (user preference)


# UI/UX Design

## Design Principles

### Mobile-First
- Responsive design for mobile devices (primary use case)
- Touch-friendly interface (large buttons, adequate spacing)
- Optimized for portrait orientation
- Works on tablets and desktop as well

### PWA Native Feel
- App-like appearance (no browser chrome when installed)
- Smooth transitions between screens



## User Flow
1. App launches → Main interface with microphone button
2. User clicks "Extract HA Config" button → Frontend connects to HA WebSocket API
3. Frontend fetches entity registry and area data → Converts to custom JSON structure
4. Custom JSON structure stored in memory and displayed in debug panel
5. User taps microphone button → Connects to AI voice assistant
6. Frontend sends custom JSON structure to Gemini as context (if available)
7. User speaks command (e.g., "turn on all lights in the kitchen")
8. User's spoken input is transcribed and displayed in UI debug log
9. AI processes audio → Generates Home Assistant control structure (JSON)
10. AI responds with audio confirmation (e.g., "turning on all lights in the kitchen") - *Note: May be inconsistent with preview model*
11. Frontend executes command directly on Home Assistant REST API
12. Function response sent back to AI via `sendToolResponse` method
13. User can disconnect or continue with more commands

## Installation Flow
- App runs in browser by default
- "Install App" button appears discretely when PWA installation is available
- Button is hidden if app is already installed or installation is not supported
- Installation is optional - user can continue using app in browser if preferred

# Future Enhancements

This section documents potential improvements and features that could be added in future versions.

## Security Enhancements

### API Key Management
- **Gemini Session Tokens**: Investigate if Gemini Live API supports temporary session tokens
  - Short-lived tokens for sessions
  - Token expires after session ends
  - Periodic token refresh
  - Token validation before use

## Performance & Scalability

### Home Assistant Integration Improvements
- **Automatic Config Refresh**: Add automatic re-fetch of HA configuration
  - Periodic refresh (e.g., every X minutes)
  - Refresh on app start
  - Background refresh while app is running
- **Real-time State Updates**: Use WebSocket API for real-time entity state monitoring
  - Lower latency for state changes
  - Better for frequent commands
  - Bidirectional communication

### Caching & Optimization
- **Audio Optimization**: Optimize audio processing
  - Consider AudioWorklet instead of ScriptProcessorNode (deprecated)
  - Better audio compression/encoding
  - Audio resampling improvements

## Features

### User Experience
- **Voice Wake Word**: Add wake word detection (e.g., "Hey Tiddeli")
- **Command History**: Store and display previous commands
- **Favorites/Shortcuts**: Quick access to frequently used commands

### Home Assistant Integration
- **Scene Support**: Support for Home Assistant scenes
- **Automation Triggers**: Trigger HA automations via voice
- **Group Management**: Voice control for device groups

### Advanced AI Features
- **Context Memory**: Remember previous commands in conversation


## Development & Operations

### Testing
- **Unit Tests**: Add unit tests for utility functions and API clients
- **Integration Tests**: Automated testing of frontend-Gemini-HA flow
- **E2E Tests**: End-to-end testing with real audio input
- **Performance Tests**: Load testing and performance benchmarking

### Monitoring & Logging
- **Error Tracking**: Frontend error logging and tracking
  - Browser console logging
  - Optional error reporting service integration
- **Analytics**: Usage analytics (commands executed, success rates)
- **Performance Monitoring**: Monitor API response times, audio latency

### Deployment
- **Static Hosting**: Deploy frontend as static files (no server needed)
- **CI/CD Pipeline**: Automated testing and deployment
- **Environment Management**: Separate dev/staging/production configs
- **Cloud Deployment**: Deployment guides for static hosting platforms (Vercel, Netlify, GitHub Pages, etc.)



